"""
SEOãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç”¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé«˜é€Ÿç‰ˆï¼‰
"""

import os
import time
import logging
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import constants as ct

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

def quick_chunk_test():
    """é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ"""
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ï¼ˆPDFãƒ•ã‚¡ã‚¤ãƒ«3ã¤ã®ã¿ï¼‰
    sample_files = [
        "2025-2026_SEO1-3_local_seo_meo.pdf",
        "2025-2026_SEO2-1_content_assets.pdf", 
        "2025-2026_SEO3-1_keyword_research.pdf"
    ]
    
    docs_all = []
    for filename in sample_files:
        file_path = f"./data/{filename}"
        if os.path.exists(file_path):
            loader = PyMuPDFLoader(file_path)
            docs = loader.load()
            docs_all.extend(docs)
            print(f"èª­ã¿è¾¼ã¿: {filename} ({len(docs)}ãƒšãƒ¼ã‚¸)")
    
    print(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(docs_all)}")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªï¼ˆ1ã¤ã®ã¿ï¼‰
    query = "MEOã§ä¸Šä½è¡¨ç¤ºã•ã›ã‚‹ãŸã‚ã®åŠ¹æœçš„ãªæ–½ç­–ã‚’æ•™ãˆã¦ãã ã•ã„"
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºå€™è£œ
    chunk_sizes = [300, 500, 700, 1000]
    
    results = []
    
    print("\nğŸ” é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆé–‹å§‹...")
    print("=" * 60)
    
    for chunk_size in chunk_sizes:
        print(f"\nğŸ“Š ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        
        start_time = time.time()
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunk_overlap = int(chunk_size * 0.15)
        text_splitter = CharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separator="\n"
        )
        splitted_docs = text_splitter.split_documents(docs_all)
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆï¼ˆä¸€æ™‚çš„ï¼‰
        embeddings = OpenAIEmbeddings()
        db = Chroma.from_documents(splitted_docs, embedding=embeddings)
        retriever = db.as_retriever(search_kwargs={"k": 3})
        
        # LLMè¨­å®š
        llm = ChatOpenAI(
            model_name=ct.MODEL,
            temperature=ct.TEMPERATURE,
            max_tokens=ct.MAX_TOKENS
        )
        
        # RAGãƒã‚§ãƒ¼ãƒ³
        chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever
        )
        
        # å›ç­”ç”Ÿæˆ
        result = chain.invoke({"query": query})
        
        processing_time = time.time() - start_time
        
        # çµæœè¨˜éŒ²
        chunk_result = {
            "chunk_size": chunk_size,
            "chunk_count": len(splitted_docs),
            "processing_time": processing_time,
            "answer_length": len(result["result"]),
            "answer": result["result"][:200] + "..." if len(result["result"]) > 200 else result["result"]
        }
        
        results.append(chunk_result)
        
        print(f"  â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"  ğŸ“„ ãƒãƒ£ãƒ³ã‚¯æ•°: {len(splitted_docs)}")
        print(f"  ğŸ’¬ å›ç­”é•·: {len(result['result'])}æ–‡å­—")
        print(f"  ğŸ“ å›ç­”ä¾‹: {chunk_result['answer']}")
    
    # çµæœã¾ã¨ã‚
    print("\n" + "=" * 60)
    print("ğŸ† æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    print(f"{'ãƒãƒ£ãƒ³ã‚¯':<8} {'æ™‚é–“':<8} {'ãƒãƒ£ãƒ³ã‚¯æ•°':<10} {'å›ç­”é•·':<8} {'è©•ä¾¡'}")
    print("-" * 50)
    
    for result in results:
        # ç°¡æ˜“è©•ä¾¡
        if result["processing_time"] <= 8 and result["answer_length"] >= 300:
            evaluation = "ğŸ¥‡ æœ€é©"
        elif result["processing_time"] <= 10:
            evaluation = "ğŸ¥ˆ è‰¯å¥½"
        else:
            evaluation = "ğŸ¥‰ æ”¹å–„è¦"
            
        print(f"{result['chunk_size']:<8} {result['processing_time']:.1f}s{'':>3} "
              f"{result['chunk_count']:<10} {result['answer_length']:<8} {evaluation}")
    
    # æ¨å¥¨è¨­å®š
    best = min(results, key=lambda x: x["processing_time"] if x["answer_length"] >= 300 else float('inf'))
    
    print(f"\nğŸ’¡ æ¨å¥¨è¨­å®š:")
    print(f"  chunk_size = {best['chunk_size']}")
    print(f"  chunk_overlap = {int(best['chunk_size'] * 0.15)}")
    print(f"  äºˆæƒ³å‡¦ç†æ™‚é–“: {best['processing_time']:.1f}ç§’")
    
    return results

if __name__ == "__main__":
    quick_chunk_test()