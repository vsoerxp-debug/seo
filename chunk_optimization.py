"""
SEOãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç”¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å›ç­”ç²¾åº¦ã¨å›ç­”ã‚¹ãƒ”ãƒ¼ãƒ‰ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æ¤œè¨¼
"""

import os
import time
import logging
from datetime import datetime
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import constants as ct

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def adjust_string(text):
    """æ–‡å­—åŒ–ã‘å¯¾ç­–ç”¨ã®æ–‡å­—åˆ—èª¿æ•´"""
    if isinstance(text, str):
        return text.replace('\u3000', ' ').replace('\xa0', ' ')
    return text

def load_seo_documents():
    """SEOé–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    docs_all = []
    data_path = "./data"
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¨DOCXãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    for filename in os.listdir(data_path):
        file_path = os.path.join(data_path, filename)
        
        if filename.endswith('.pdf'):
            loader = PyMuPDFLoader(file_path)
            docs = loader.load()
            docs_all.extend(docs)
            logger.info(f"PDFèª­ã¿è¾¼ã¿: {filename} ({len(docs)}ãƒšãƒ¼ã‚¸)")
            
        elif filename.endswith('.docx'):
            loader = Docx2txtLoader(file_path)
            docs = loader.load()
            docs_all.extend(docs)
            logger.info(f"DOCXèª­ã¿è¾¼ã¿: {filename} ({len(docs)}ãƒ•ã‚¡ã‚¤ãƒ«)")
    
    # æ–‡å­—åŒ–ã‘å¯¾ç­–
    for doc in docs_all:
        doc.page_content = adjust_string(doc.page_content)
        for key in doc.metadata:
            doc.metadata[key] = adjust_string(doc.metadata[key])
    
    logger.info(f"ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(docs_all)}")
    return docs_all

def generate_seo_response(chunk_size, chunk_overlap_ratio, docs_all, query, k_search=5):
    """æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§SEOå›ç­”ã‚’ç”Ÿæˆ"""
    start_time = time.time()
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    chunk_overlap = int(chunk_size * chunk_overlap_ratio)
    text_splitter = CharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separator="\n"
    )
    splitted_docs = text_splitter.split_documents(docs_all)
    
    # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ
    embeddings = OpenAIEmbeddings()
    db = Chroma.from_documents(splitted_docs, embedding=embeddings)
    retriever = db.as_retriever(search_kwargs={"k": k_search})
    
    # LLMè¨­å®šï¼ˆç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜ï¼‰
    llm = ChatOpenAI(
        model_name=ct.MODEL,
        temperature=ct.TEMPERATURE,
        max_tokens=ct.MAX_TOKENS
    )
    
    # RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆ
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    
    # å›ç­”ç”Ÿæˆ
    result = chain.invoke({"query": query})
    
    # å‡¦ç†æ™‚é–“è¨ˆç®—
    processing_time = time.time() - start_time
    
    return {
        "answer": result["result"],
        "processing_time": processing_time,
        "chunk_count": len(splitted_docs),
        "source_docs": len(result["source_documents"]),
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap
    }

def run_chunk_optimization():
    """ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œ"""
    
    # SEOãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    print("ğŸ“ SEOãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
    docs_all = load_seo_documents()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªï¼ˆSEOå°‚ç”¨ï¼‰
    test_queries = [
        "MEOã§ä¸Šä½è¡¨ç¤ºã•ã›ã‚‹ãŸã‚ã®åŠ¹æœçš„ãªæ–½ç­–ã‚’æ•™ãˆã¦ãã ã•ã„",
        "SEOã«ãŠã‘ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸å®šã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹",
        "Googleã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®å½±éŸ¿ã¨ãã®å¯¾ç­–ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"
    ]
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æ¤œè¨¼ç¯„å›²
    chunk_configs = [
        {"size": 200, "overlap_ratio": 0.2},   # å°ã•ã‚ï¼ˆè©³ç´°é‡è¦–ï¼‰
        {"size": 400, "overlap_ratio": 0.2},   # ç¾åœ¨ã‚ˆã‚Šå°ã•ã‚
        {"size": 500, "overlap_ratio": 0.16},  # ç¾åœ¨ã®è¨­å®š
        {"size": 600, "overlap_ratio": 0.15},  # ã‚„ã‚„å¤§ãã‚
        {"size": 800, "overlap_ratio": 0.125}, # å¤§ãã‚ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé‡è¦–ï¼‰
        {"size": 1000, "overlap_ratio": 0.1},  # æœ€å¤§ã‚µã‚¤ã‚º
    ]
    
    results = []
    
    print("\nğŸ” ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    print("=" * 80)
    
    for config in chunk_configs:
        chunk_size = config["size"]
        overlap_ratio = config["overlap_ratio"]
        
        print(f"\nğŸ“Š ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}, ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”: {overlap_ratio:.1%}")
        print("-" * 60)
        
        query_results = []
        
        for i, query in enumerate(test_queries, 1):
            print(f"  ã‚¯ã‚¨ãƒª {i}: {query[:30]}...")
            
            try:
                result = generate_seo_response(
                    chunk_size=chunk_size,
                    chunk_overlap_ratio=overlap_ratio,
                    docs_all=docs_all,
                    query=query
                )
                
                query_results.append(result)
                print(f"    â±ï¸  å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
                print(f"    ğŸ“„ ãƒãƒ£ãƒ³ã‚¯æ•°: {result['chunk_count']}")
                print(f"    ğŸ” å‚ç…§æ–‡æ›¸: {result['source_docs']}")
                print(f"    ğŸ’¬ å›ç­”é•·: {len(result['answer'])}æ–‡å­—")
                
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å¹³å‡å€¤è¨ˆç®—
        if query_results:
            avg_time = sum(r['processing_time'] for r in query_results) / len(query_results)
            avg_chunks = query_results[0]['chunk_count']  # ãƒãƒ£ãƒ³ã‚¯æ•°ã¯åŒã˜
            avg_answer_length = sum(len(r['answer']) for r in query_results) / len(query_results)
            
            summary = {
                "chunk_size": chunk_size,
                "overlap_ratio": overlap_ratio,
                "avg_processing_time": avg_time,
                "chunk_count": avg_chunks,
                "avg_answer_length": avg_answer_length,
                "speed_score": 10 - min(avg_time, 10),  # 10ç§’ä»¥å†…ã§é«˜ã‚¹ã‚³ã‚¢
                "detail_score": min(avg_answer_length / 100, 10)  # å›ç­”ã®è©³ç´°åº¦
            }
            
            # ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé€Ÿåº¦50%, è©³ç´°åº¦50%ï¼‰
            summary["balance_score"] = (summary["speed_score"] * 0.5 + 
                                      summary["detail_score"] * 0.5)
            
            results.append(summary)
            
            print(f"  ğŸ“ˆ å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.2f}ç§’")
            print(f"  ğŸ“Š ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {summary['balance_score']:.2f}/10")
    
    # çµæœã¾ã¨ã‚
    print("\n" + "=" * 80)
    print("ğŸ† æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    
    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x["balance_score"], reverse=True)
    
    print(f"{'ãƒ©ãƒ³ã‚¯':<4} {'ãƒãƒ£ãƒ³ã‚¯':<6} {'é‡è¤‡æ¯”':<6} {'æ™‚é–“':<6} {'ã‚¹ã‚³ã‚¢':<6} {'æ¨å¥¨ç†ç”±'}")
    print("-" * 60)
    
    for i, result in enumerate(results[:3], 1):
        reason = ""
        if i == 1:
            if result["avg_processing_time"] <= 8:
                reason = "æœ€é©ãƒãƒ©ãƒ³ã‚¹ğŸ¥‡"
            else:
                reason = "é«˜å“è³ªé‡è¦–ğŸ¥‡"
        elif i == 2:
            reason = "æº–æœ€é©ğŸ¥ˆ"
        else:
            reason = "ä»£æ›¿æ¡ˆğŸ¥‰"
            
        print(f"{i:<4} {result['chunk_size']:<6} {result['overlap_ratio']:.1%}{'':>1} "
              f"{result['avg_processing_time']:.1f}s{'':>1} {result['balance_score']:.1f}{'':>2} {reason}")
    
    # æ¨å¥¨è¨­å®š
    best_config = results[0]
    print(f"\nğŸ’¡ æ¨å¥¨è¨­å®š:")
    print(f"  chunk_size = {best_config['chunk_size']}")
    print(f"  chunk_overlap = {int(best_config['chunk_size'] * best_config['overlap_ratio'])}")
    print(f"  æœŸå¾…å‡¦ç†æ™‚é–“: {best_config['avg_processing_time']:.1f}ç§’")

if __name__ == "__main__":
    run_chunk_optimization()